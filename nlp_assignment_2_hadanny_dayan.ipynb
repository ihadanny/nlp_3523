{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_assignment_2_hadanny_dayan",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ihadanny/nlp_3523/blob/master/nlp_assignment_2_hadanny_dayan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0zB2BpE6DhW",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 2\n",
        "\n",
        "This assignment is about training and evaluating a POS tagger with some real data. The dataset is available through NLTK, a Python NLP package."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UH-Xvqip6Teu",
        "colab_type": "text"
      },
      "source": [
        "**Part 1** (no actions required)\n",
        "\n",
        "The dataset is composed of a set of sentences. Each sentence is a list of tuples of a word and a tag, as provided by human annotators.\n",
        "You should split the data to train and test sets in the following way:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsZsyTVC6Sw0",
        "colab_type": "code",
        "outputId": "377eb377-b22e-41dd-e172-718f0b5b8f38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "import numpy as np\n",
        "import operator\n",
        "import nltk\n",
        "from nltk.corpus import treebank \n",
        "nltk.download('treebank')\n",
        "print(f\"Number of sentences: {len(treebank.tagged_sents())}\")\n",
        "\n",
        "train_data = treebank.tagged_sents()[:3000] \n",
        "test_data = treebank.tagged_sents()[3000:] \n",
        "print(train_data[0])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "Number of sentences: 3914\n",
            "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0Z9BMNM7EP3",
        "colab_type": "text"
      },
      "source": [
        "**Part 2**\n",
        "\n",
        "Write a class simple_tagger, with methods *train* and *evaluate*. The method *train* receives the data as a list of treebank sentences, as presented below, and use it for training the tagger. In this case, it should learn a simple map of words to tags, defined as the most frequent tag for every word (in case there is more than one tag, select one randomly). The map should be stored as a class member for evaluation.\n",
        "\n",
        "The method evaluate receives the data as a list of treebanl sentences, as presented above, and use it to evaluate the tagger performance. Specifically, it should calculate the word and sentence level accuracy.\n",
        "The evaluation process is simply going word by word, querying the map (created by the train method) for each word’s tag and compare it to the true tag of that word. The word-level accuracy is the number of successes divided by the number of words. For OOV (out of vocabulary, or unknown) words, the tagger should assign the most frequent tag in the entire training set. The function should return the two numbers: word level accuracy and sentence level accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtivZLBH7dXq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "class simple_tagger:\n",
        "  def train(self, data):\n",
        "    occs = [x for sent in data for x in sent]\n",
        "    occs = collections.Counter(occs)\n",
        "    freq = collections.defaultdict(list)\n",
        "    for (word, tag), cnt in occs.items():\n",
        "      freq[word].append((cnt, tag))\n",
        "    self.most_freq_tag_per_word = {word: max(pos_list)[1] for word, pos_list in freq.items()}\n",
        "    all_tags = [tag for sent in data for word, tag in sent]\n",
        "    all_tags = collections.Counter(all_tags)\n",
        "    self.most_freq_tag_overall = all_tags.most_common(1)\n",
        "\n",
        "  def evaluate(self, data):\n",
        "    word_successes, sent_successes = 0, 0\n",
        "    for sent in data:\n",
        "      entire_sent_success = True\n",
        "      for word, true_tag in sent:\n",
        "        tag = self.most_freq_tag_per_word[word] if word in self.most_freq_tag_per_word else self.most_freq_tag_overall\n",
        "        if tag == true_tag:\n",
        "          word_successes += 1\n",
        "        else: \n",
        "          entire_sent_success = False\n",
        "      if entire_sent_success:\n",
        "        sent_successes += 1\n",
        "    words, sents = len([word for sent in data for word in sent]), len(data)\n",
        "    return word_successes / words, sent_successes / sents\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5XLUe8Xyr2m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d7f76b4a-6f8f-40d4-e72e-5a39c451164a"
      },
      "source": [
        "tagger = simple_tagger()\n",
        "tagger.train(train_data)\n",
        "tagger.evaluate(test_data)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.8584502482192964, 0.07330415754923414)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etK9iZIq8i0X",
        "colab_type": "text"
      },
      "source": [
        "**Part 3**\n",
        "\n",
        "Similar to part 2, write the class hmm_tagger, which implements HMM tagging. The method *train* should build the matrices A, B and Pi, from the data as discussed in class. The method *evaluate* should find the best tag sequence for every input sentence, using he Viterbi decoding algorithm, and then calculate the word and sentence level accuracy using the gold-standard tags. I implemented the Viterbi algorithm for you in the next block, so you can should either plug it into your code or write your own Viterbi version.\n",
        "\n",
        "Additional guidance:\n",
        "1. The matrix B represents the probabilities of seeing a word within each POS label.\n",
        "Since B is a matrix, you should build a dictionary that maps every unique word in the corpus to a serial numeric id (starting with 0). This way, every column in B represents the word that it’s id matches the index of the column.\n",
        "2. During evaluation, you should first convert each word into it’s index and create the observation array to be given to Viterbi, as a list of ids. OOV words should be assigned with a random tag. To make sure Viterbi works appropriately, you can simply break the sentence into multiple segments every time you see OOV word, and decode every segment individually by Viterbi.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DR6KJW2F9yqt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Viterbi\n",
        "def viterbi (word_list, A, B, Pi):\n",
        "\n",
        "    # initialization\n",
        "    T = len(word_list)\n",
        "    N = A.shape[0] # number of tags\n",
        "\n",
        "    delta_table = np.zeros((N, T)) # initialise delta table\n",
        "    psi = np.zeros((N, T))  # initialise the best path table\n",
        "\n",
        "    delta_table[:,0] = B[:, word_list[0]] * Pi\n",
        "\n",
        "    for t in range(1, T):\n",
        "        for s in range (0, N):\n",
        "            trans_p = delta_table[:, t-1] * A[:, s]\n",
        "            psi[s, t], delta_table[s, t] = max(enumerate(trans_p), key=operator.itemgetter(1))\n",
        "            delta_table[s, t] = delta_table[s, t] * B[s, word_list[t]]\n",
        "\n",
        "    # Back tracking\n",
        "    seq = np.zeros(T);\n",
        "    seq[T-1] =  delta_table[:, T-1].argmax()\n",
        "    for t in range(T-1, 0, -1):\n",
        "      #print(seq[t])\n",
        "      seq[t-1] = psi[int(seq[t]),t]\n",
        "\n",
        "    return seq\n",
        "\n",
        "# A simple example to run the algorithm:\n",
        "\n",
        "# A = np.array([[0.3, 0.7], [0.2, 0.8]])\n",
        "# B = np.array([[0.1, 0.1, 0.3, 0.5], [0.3, 0.3, 0.2, 0.2]])\n",
        "# Pi = np.array([0.4, 0.6])\n",
        "# print(viterbi([3, 3, 3, 3], A, B, Pi))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpH7GuiQ9L6W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "class hmm_tagger:\n",
        "  def sent_to_indexes(self, sent):\n",
        "    res = []\n",
        "    for word, tag in sent:\n",
        "      tag = self.tags[tag]\n",
        "      word = self.words[word] if word in self.words else random.randint(0, len(self.words)-1)\n",
        "      res.append((word, tag))\n",
        "    return res\n",
        "\n",
        "  def train(self, data):\n",
        "    self.words = set([word for sent in data for word, tag in sent])\n",
        "    self.words = {word: idx for idx, word in enumerate(self.words)}\n",
        "    self.tags = set([tag for sent in data for word, tag in sent])\n",
        "    self.tags = {tag: idx for idx, tag in enumerate(self.tags)}\n",
        "\n",
        "    N, T = len(self.tags), len(self.words)\n",
        "    self.A, self.B, self.Pi = np.zeros((N, N)), np.zeros((N, T)), np.zeros(N)\n",
        "\n",
        "    for sent in data:\n",
        "      sent = self.sent_to_indexes(sent)\n",
        "      for idx, (word,tag) in enumerate(sent):\n",
        "        self.B[tag, word] += 1\n",
        "        if idx == 0:\n",
        "          self.Pi[tag] += 1\n",
        "        else:\n",
        "          _, prev_tag = sent[idx-1]\n",
        "          self.A[prev_tag, tag] += 1\n",
        "\n",
        "    def norm_by_row(mat):\n",
        "      return (mat.T / mat.T.sum(axis = 0)).T\n",
        "    self.A, self.B, self.Pi = norm_by_row(self.A), norm_by_row(self.B), norm_by_row(self.Pi) \n",
        "\n",
        "  def evaluate(self, data):\n",
        "    N, T = len(self.tags), len(self.words)\n",
        "\n",
        "    word_successes, sent_successes = 0, 0\n",
        "    for sent in data:\n",
        "      sent = self.sent_to_indexes(sent)\n",
        "      words = [word for word, tag in sent]\n",
        "      true_tags =[tag for word, tag in sent]\n",
        "      tags = viterbi(words, self.A, self.B, self.Pi)\n",
        "\n",
        "      entire_sent_success = True\n",
        "      for idx in range(len(tags)):\n",
        "        if tags[idx] == true_tags[idx]:\n",
        "          word_successes += 1\n",
        "        else: \n",
        "          entire_sent_success = False\n",
        "      if entire_sent_success:\n",
        "        sent_successes += 1\n",
        "    words, sents = len([word for sent in data for word in sent]), len(data)\n",
        "    return word_successes / words, sent_successes / sents\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNbTcyrGF0Go",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b39ad206-c276-4ad6-89f9-1a2facc94ec5"
      },
      "source": [
        "\n",
        "tagger = hmm_tagger()\n",
        "tagger.train(train_data)\n",
        "tagger.evaluate(test_data)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7890783509605007, 0.1050328227571116)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YZO0uGL-4S-",
        "colab_type": "text"
      },
      "source": [
        "**Part 4**\n",
        "\n",
        "Compare the results obtained from both taggers and a MEMM tagger, implemented by NLTK, over the test data. To train the NLTK MEMM tagger you should execute the following lines (it may take some time to train...):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYhtboJm_Iyx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "149b8ea1-afb8-4186-a3bc-b4efeba65590"
      },
      "source": [
        "from nltk.tag import tnt \n",
        "\n",
        "tnt_pos_tagger = tnt.TnT()\n",
        "tnt_pos_tagger.train(train_data)\n",
        "print(tnt_pos_tagger.evaluate(test_data))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.875545003237643\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DIvvzsq_U-o",
        "colab_type": "text"
      },
      "source": [
        "TODO: Print both, word level and sentence level accuracy for all the three taggers in a table."
      ]
    }
  ]
}